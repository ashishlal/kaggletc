{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../data/glove.840B.300d.txt'\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = train['toxic'].values\n",
    "y2 = train['severe_toxic'].values\n",
    "y3 = train['obscene'].values\n",
    "y4 = train['threat'].values\n",
    "y5 = train['insult'].values\n",
    "y6 = train['identity_hate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Attention import *\n",
    "\n",
    "def build_model_nn1(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NN1(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN1_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn1(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "\n",
    "def build_model_nn2(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = AttentionWithContext()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN2_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn2(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "from Attention import *\n",
    "\n",
    "def build_model_nn3(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att1 = AttentionWithContext()(x)\n",
    "    att2 = Attention()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att1, att2]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN3_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn3(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "from Attention import *\n",
    "\n",
    "def build_model_nn4(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN4(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN4_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn4(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nn5(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_sp_dr=0.2, \n",
    "                    optim=Adam(lr=0.001)):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optim,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN5(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_sp_dr=0.2,\n",
    "                 batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        \n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN5_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn5(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((10,6))\n",
    "c = np.random.random((10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((10, 6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:,:,0] = a\n",
    "b[:,:,1] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17727482, 0.45456644, 0.49212607, 0.50721638, 0.50686121,\n",
       "        0.161148  ],\n",
       "       [0.20563254, 0.64988956, 0.32744716, 0.17360135, 0.35707627,\n",
       "        0.48103542],\n",
       "       [0.44689288, 0.19128629, 0.53710537, 0.41709411, 0.37870518,\n",
       "        0.49350407],\n",
       "       [0.31327662, 0.16441258, 0.38323538, 0.21965772, 0.29665035,\n",
       "        0.52717279],\n",
       "       [0.40057884, 0.22551249, 0.48584782, 0.37296569, 0.19768885,\n",
       "        0.57517161],\n",
       "       [0.43608877, 0.34935421, 0.22674936, 0.61298084, 0.02550516,\n",
       "        0.25646376],\n",
       "       [0.34728608, 0.28166745, 0.25873408, 0.46843744, 0.28097777,\n",
       "        0.28730284],\n",
       "       [0.34534859, 0.30243579, 0.34651773, 0.37009074, 0.27523783,\n",
       "        0.2359247 ],\n",
       "       [0.18054381, 0.51721435, 0.22347868, 0.22697491, 0.38112764,\n",
       "        0.18070692],\n",
       "       [0.47845215, 0.44294616, 0.19088375, 0.28891556, 0.27543041,\n",
       "        0.41902096]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "final_score = 0.0\n",
    "n_classes = 6\n",
    "class Ensemble1(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "#         folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "#                                                              random_state=2016).split(X, y))\n",
    "        \n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=233)\n",
    "        OOF_columns = []\n",
    "\n",
    "#         S_train = np.zeros((X.shape[0], len(self.base_models)) * n_classes)\n",
    "#         S_test = np.zeros((T.shape[0], len(self.base_models)) * n_classes)\n",
    "        \n",
    "        S_train = np.zeros((X.shape[0], n_classes, len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], n_classes, len(self.base_models)))\n",
    "        \n",
    "#         S_train = []\n",
    "#         S_test = []\n",
    "#         (159571, 6)\n",
    "#         (159571, 2)\n",
    "#         print(y.shape)\n",
    "        print(S_train.shape)\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], n_classes, self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                print(test_idx.shape)\n",
    "                print(train_idx.shape)\n",
    "                print(test_idx.shape[0])\n",
    "                print(train_idx.shape[0])\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "#                 S_train[0:test_idx[0], n_classes, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_train[j*test_idx.shape[0]:(j+1)*test_idx.shape[0], :, i] = clf.predict_proba(X_holdout)\n",
    "                S_test_i[:, :, j] = clf.predict_proba(T)                \n",
    "            S_test[:, :, i] = S_test_i.mean(axis=2)\n",
    "            \n",
    "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,:,i])))\n",
    "#             print(\"  Base model_%d accuracy score: %.5f\\n\" % (i+1, accuracy_score(y, S_train[:,:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "#         OOF_S_train = pd.DataFrame(np.array(S_train), columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\n",
    "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,\n",
    "#                                                             accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "#             print(\"2nd level model final accuracy score: %.5f\" % (accuracy_score(y, stack_score)))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))    \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "#             print(\"2nd level models final accuracy score: %.5f\" % (accuracy_score(y, stack_score.mean(axis=1))))\n",
    "            final_score = accuracy_score(y, stack_score.mean(axis=1))\n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "final_score = 0.0\n",
    "class Ensemble(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        OOF_columns = []\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                S_train[test_idx, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]                \n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "            \n",
    "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\n",
    "#             print(\"  Base model_%d accuracy score: %.5f\\n\" % (i+1, accuracy_score(y, S_train[:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\n",
    "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,\n",
    "#                                                             accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "#             print(\"2nd level model final accuracy score: %.5f\" % (accuracy_score(y, stack_score)))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))    \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "#             print(\"2nd level models final accuracy score: %.5f\" % (accuracy_score(y, stack_score.mean(axis=1))))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))\n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = NN1()\n",
    "nn2 = NN2()\n",
    "nn3 = NN3()\n",
    "nn4 = NN4()\n",
    "nn5 = NN5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacker models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "et_model = ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_split=10, random_state=10)\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit NN3_1 fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9803\n",
      " ROC-AUC - epoch: 1 - score: 0.986334\n",
      "Epoch 00000: val_acc improved from -inf to 0.98161, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 739s - loss: 0.0570 - acc: 0.9803 - val_loss: 0.0465 - val_acc: 0.9816\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9832\n",
      " ROC-AUC - epoch: 2 - score: 0.987396\n",
      "Epoch 00001: val_acc improved from 0.98161 to 0.98316, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 755s - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0434 - val_acc: 0.9832\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9840\n",
      " ROC-AUC - epoch: 3 - score: 0.988717\n",
      "Epoch 00002: val_acc improved from 0.98316 to 0.98377, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 773s - loss: 0.0411 - acc: 0.9840 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847\n",
      " ROC-AUC - epoch: 4 - score: 0.988814\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 766s - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0420 - val_acc: 0.9837\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9853\n",
      " ROC-AUC - epoch: 5 - score: 0.989330\n",
      "Epoch 00004: val_acc improved from 0.98377 to 0.98404, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 763s - loss: 0.0372 - acc: 0.9853 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Predicting....\n",
      "79786/79786 [==============================] - 161s   \n",
      "(79786, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 311s   \n",
      "(153164, 6)\n",
      "Fit NN3_1 fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9810\n",
      " ROC-AUC - epoch: 1 - score: 0.986018\n",
      "Epoch 00000: val_acc improved from -inf to 0.98255, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 767s - loss: 0.0537 - acc: 0.9810 - val_loss: 0.0449 - val_acc: 0.9825\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9836\n",
      " ROC-AUC - epoch: 2 - score: 0.988258\n",
      "Epoch 00001: val_acc improved from 0.98255 to 0.98348, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 758s - loss: 0.0426 - acc: 0.9836 - val_loss: 0.0421 - val_acc: 0.9835\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9845\n",
      " ROC-AUC - epoch: 3 - score: 0.988349\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 758s - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0437 - val_acc: 0.9831\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9853\n",
      " ROC-AUC - epoch: 4 - score: 0.988196\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 758s - loss: 0.0374 - acc: 0.9853 - val_loss: 0.0445 - val_acc: 0.9833\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860\n",
      " ROC-AUC - epoch: 5 - score: 0.988574\n",
      "Epoch 00004: val_acc improved from 0.98348 to 0.98368, saving model to ../cache/1_18_NN3_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 758s - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0435 - val_acc: 0.9837\n",
      "Predicting....\n",
      "79785/79785 [==============================] - 161s   \n",
      "(79785, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 309s   \n",
      "(153164, 6)\n",
      "  Base model_1 score: 0.97975\n",
      "\n",
      "Fit NN4_2 fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9798\n",
      " ROC-AUC - epoch: 1 - score: 0.986189\n",
      "Epoch 00000: val_acc improved from -inf to 0.98294, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 746s - loss: 0.0579 - acc: 0.9798 - val_loss: 0.0441 - val_acc: 0.9829\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9833\n",
      " ROC-AUC - epoch: 2 - score: 0.988254\n",
      "Epoch 00001: val_acc did not improve\n",
      "143613/143613 [==============================] - 731s - loss: 0.0439 - acc: 0.9833 - val_loss: 0.0446 - val_acc: 0.9828\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9840\n",
      " ROC-AUC - epoch: 3 - score: 0.989187\n",
      "Epoch 00002: val_acc improved from 0.98294 to 0.98396, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 730s - loss: 0.0411 - acc: 0.9840 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9847\n",
      " ROC-AUC - epoch: 4 - score: 0.988858\n",
      "Epoch 00003: val_acc improved from 0.98396 to 0.98407, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 724s - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0422 - val_acc: 0.9841\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854\n",
      " ROC-AUC - epoch: 5 - score: 0.989063\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 721s - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0420 - val_acc: 0.9836\n",
      "Predicting....\n",
      "79786/79786 [==============================] - 155s   \n",
      "(79786, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 298s   \n",
      "(153164, 6)\n",
      "Fit NN4_2 fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9813\n",
      " ROC-AUC - epoch: 1 - score: 0.987693\n",
      "Epoch 00000: val_acc improved from -inf to 0.98379, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 724s - loss: 0.0516 - acc: 0.9813 - val_loss: 0.0430 - val_acc: 0.9838\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9838\n",
      " ROC-AUC - epoch: 2 - score: 0.988605\n",
      "Epoch 00001: val_acc did not improve\n",
      "143613/143613 [==============================] - 716s - loss: 0.0419 - acc: 0.9838 - val_loss: 0.0422 - val_acc: 0.9833\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9847\n",
      " ROC-AUC - epoch: 3 - score: 0.989411\n",
      "Epoch 00002: val_acc improved from 0.98379 to 0.98397, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 717s - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9855\n",
      " ROC-AUC - epoch: 4 - score: 0.989092\n",
      "Epoch 00003: val_acc improved from 0.98397 to 0.98412, saving model to ../cache/1_18_NN4_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 718s - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9863\n",
      " ROC-AUC - epoch: 5 - score: 0.988856\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 718s - loss: 0.0345 - acc: 0.9863 - val_loss: 0.0427 - val_acc: 0.9838\n",
      "Predicting....\n",
      "79785/79785 [==============================] - 155s   \n",
      "(79785, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 297s   \n",
      "(153164, 6)\n",
      "  Base model_2 score: 0.97372\n",
      "\n",
      "Fit NN5_3 fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9804\n",
      " ROC-AUC - epoch: 1 - score: 0.985985\n",
      "Epoch 00000: val_acc improved from -inf to 0.98354, saving model to ../cache/1_18_NN5_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1006s - loss: 0.0559 - acc: 0.9804 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9848\n",
      " ROC-AUC - epoch: 2 - score: 0.988699\n",
      "Epoch 00001: val_acc improved from 0.98354 to 0.98391, saving model to ../cache/1_18_NN5_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 996s - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9870\n",
      " ROC-AUC - epoch: 3 - score: 0.988090\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 1003s - loss: 0.0323 - acc: 0.9870 - val_loss: 0.0443 - val_acc: 0.9838\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9893\n",
      " ROC-AUC - epoch: 4 - score: 0.986875\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 998s - loss: 0.0268 - acc: 0.9893 - val_loss: 0.0485 - val_acc: 0.9835\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9913\n",
      " ROC-AUC - epoch: 5 - score: 0.985654\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 999s - loss: 0.0221 - acc: 0.9913 - val_loss: 0.0536 - val_acc: 0.9821\n",
      "Predicting....\n",
      "79786/79786 [==============================] - 147s   \n",
      "(79786, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 283s   \n",
      "(153164, 6)\n",
      "Fit NN5_3 fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9813\n",
      " ROC-AUC - epoch: 1 - score: 0.986356\n",
      "Epoch 00000: val_acc improved from -inf to 0.98311, saving model to ../cache/1_18_NN5_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1001s - loss: 0.0504 - acc: 0.9813 - val_loss: 0.0437 - val_acc: 0.9831\n",
      "Epoch 2/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858\n",
      " ROC-AUC - epoch: 2 - score: 0.988112\n",
      "Epoch 00001: val_acc improved from 0.98311 to 0.98359, saving model to ../cache/1_18_NN5_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1008s - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 3/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9880\n",
      " ROC-AUC - epoch: 3 - score: 0.987886\n",
      "Epoch 00002: val_acc did not improve\n",
      "143613/143613 [==============================] - 1012s - loss: 0.0298 - acc: 0.9880 - val_loss: 0.0436 - val_acc: 0.9832\n",
      "Epoch 4/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9902\n",
      " ROC-AUC - epoch: 4 - score: 0.986190\n",
      "Epoch 00003: val_acc did not improve\n",
      "143613/143613 [==============================] - 1012s - loss: 0.0246 - acc: 0.9902 - val_loss: 0.0485 - val_acc: 0.9828\n",
      "Epoch 5/5\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9922\n",
      " ROC-AUC - epoch: 5 - score: 0.985204\n",
      "Epoch 00004: val_acc did not improve\n",
      "143613/143613 [==============================] - 1013s - loss: 0.0199 - acc: 0.9922 - val_loss: 0.0553 - val_acc: 0.9822\n",
      "Predicting....\n",
      "79785/79785 [==============================] - 149s   \n",
      "(79785, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 284s   \n",
      "(153164, 6)\n",
      "  Base model_3 score: 0.98830\n",
      "\n",
      "\n",
      "\n",
      "Correlation between out-of-fold predictions from Base models:\n",
      "\n",
      "\n",
      "              Base model_1  Base model_2  Base model_3\n",
      "Base model_1      1.000000      0.920163      0.919484\n",
      "Base model_2      0.920163      1.000000      0.897421\n",
      "Base model_3      0.919484      0.897421      1.000000\n",
      "\n",
      "\n",
      "Fit LogisticRegression_1 subset 1\n",
      "Fit LogisticRegression_1 subset 2\n",
      "Fit LogisticRegression_1 subset 3\n",
      "  1st level model_1 score: 0.98654\n",
      "\n",
      "Fit ExtraTreesClassifier_2 subset 1\n",
      "Fit ExtraTreesClassifier_2 subset 2\n",
      "Fit ExtraTreesClassifier_2 subset 3\n",
      "  1st level model_2 score: 0.98572\n",
      "\n",
      "Fit MLPClassifier_3 subset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit MLPClassifier_3 subset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit MLPClassifier_3 subset 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1st level model_3 score: 0.98631\n",
      "\n",
      "\n",
      "\n",
      "Correlation between without-one-column predictions from 1st level models:\n",
      "\n",
      "\n",
      "                   1st level model_1  1st level model_2  1st level model_3\n",
      "1st level model_1           1.000000           0.979033           0.857521\n",
      "1st level model_2           0.979033           1.000000           0.926970\n",
      "1st level model_3           0.857521           0.926970           1.000000\n",
      "\n",
      "\n",
      "Fit LogisticRegression_1\n",
      "  2nd level model_1 score: 0.98666\n",
      "\n",
      "Fit ExtraTreesClassifier_2\n",
      "  2nd level model_2 score: 0.98654\n",
      "\n",
      "\n",
      "\n",
      "Correlation between final predictions from 2nd level models:\n",
      "\n",
      "\n",
      "                   2nd level model_1  2nd level model_2\n",
      "2nd level model_1           1.000000           0.988861\n",
      "2nd level model_2           0.988861           1.000000\n",
      "\n",
      "\n",
      "2nd level models final score: 0.98658\n"
     ]
    }
   ],
   "source": [
    "# Mode 2 run\n",
    "stack = Ensemble(mode=2,\n",
    "        n_splits=2,\n",
    "        stacker_2 = (log_model, et_model),         \n",
    "        stacker_1 = (log_model, et_model, mlp_model),\n",
    "        base_models = (\n",
    "            nn3,nn4,nn5\n",
    "            \n",
    "        ))       \n",
    "        \n",
    "y_pred1 = stack.fit_predict(x_train, y1, x_test)\n",
    "# y_pred2 = stack.fit_predict(x_train, y2, x_test)\n",
    "# y_pred3 = stack.fit_predict(x_train, y3, x_test)\n",
    "# y_pred4 = stack.fit_predict(x_train, y4, x_test)\n",
    "# y_pred5 = stack.fit_predict(x_train, y5, x_test)\n",
    "# y_pred6 = stack.fit_predict(x_train, y6, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = '0.98658'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-2cd615c0bb99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"toxic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"severe_toxic\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"obscene\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"threat\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"insult\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"identity_hate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../submission/sub_1_18.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Submission from mode 2\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission[list_classes] = y_pred\n",
    "sample_submission.to_csv('../submission/sub_1_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train = np.zeros((x_train.shape[0], x_train.shape[1], 2))\n",
    "# S_test = np.zeros((T.shape[0], T.shape[1], len(self.base_models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.zeros((79786, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train[x1.shape[0],x1.shape[1],0] = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktc",
   "language": "python",
   "name": "ktc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
