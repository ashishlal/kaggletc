{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from collections import Counter\n",
    "# from itertools import izip\n",
    "import operator\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "__authors__ = ['bowenwu']\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_str(sen):\n",
    "    sen = text.re.sub('[^a-zA-Z0-9]', ' ', sen)\n",
    "    sen = nltk.word_tokenize(sen.lower())\n",
    "    sen = map(snowball_stemmer.stem, sen)\n",
    "    sen = map(wordnet_lemmatizer.lemmatize, sen)\n",
    "    return (' '.join(sen)).lower()\n",
    "\n",
    "\n",
    "def preprocess(sentences):\n",
    "    new_sens = []\n",
    "    for sen in sentences:\n",
    "        new_sen = sen.replace(' i ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' you ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' he ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' she ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' him ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' me ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' they ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' them ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' us ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' our ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' my ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' your ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' his ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' her ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' their ', ' <U> ')\n",
    "        new_sen = new_sen.replace(' s ', ' <is> ')\n",
    "        new_sen = new_sen.replace(' is ', ' <is> ')\n",
    "        new_sen = new_sen.replace(' am ', ' <is> ')\n",
    "        new_sen = new_sen.replace(' are ', ' <is> ')\n",
    "        new_sen = new_sen.replace(' don t ', ' do not ')\n",
    "        new_sens.append(new_sen)\n",
    "    return new_sens\n",
    "\n",
    "def mining_pattern_withkprefix(sentences, firstk=1, topk=100, prefixs={}):\n",
    "    logger.info('Mining pattern for first %d among %d', firstk, len(sentences))\n",
    "    counts = Counter()\n",
    "    for sen in sentences:\n",
    "        words = tuple(sen.split()[:firstk])\n",
    "        if prefixs and words[:-1] not in prefixs:\n",
    "            continue\n",
    "        counts[words] += 1\n",
    "    lists = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    new_prefixs = {}\n",
    "    for words, count in lists[:topk]:\n",
    "        new_prefixs[words] = count\n",
    "    logger.info('\\tfind %d', len(new_prefixs))\n",
    "    return new_prefixs\n",
    "\n",
    "\n",
    "def mining_pattern(sentences, firstk, topk, fout, less2backup=3):\n",
    "    prefixs = {}\n",
    "    new_prefixs = {}\n",
    "    for i in range(1, firstk + 1):\n",
    "        new_prefixs = mining_pattern_withkprefix(sentences, firstk=i, topk=topk, prefixs=new_prefixs)\n",
    "        if i >= less2backup:\n",
    "            for words, count in new_prefixs.items():\n",
    "                if words[:-1] in prefixs and float(count) / prefixs[words[:-1]] > 0.8:\n",
    "                    del prefixs[words[:-1]]\n",
    "                prefixs[words] = count\n",
    "    with open(fout, 'w') as fo:\n",
    "        lists = sorted(prefixs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        for words, count in lists:\n",
    "            fo.write(str(count) + '\\t' + ' '.join(words) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patterns(fname):\n",
    "    prefixs = []\n",
    "    with open(fname, 'r') as fp:\n",
    "        for line in fp:\n",
    "            count, words = line.strip().split('\\t')\n",
    "            prefixs.append(words)\n",
    "    return prefixs\n",
    "\n",
    "\n",
    "def gen_feature_one_hot(q1s, prefixs=[]):\n",
    "    features = []\n",
    "    features_raw = []\n",
    "    feautres_col = []\n",
    "    total_features = len(prefixs)\n",
    "    for raw, q1 in enumerate(q1s):\n",
    "        for i, prefix in enumerate(prefixs):\n",
    "            if prefix in q1:\n",
    "                features.append(1)\n",
    "                features_raw.append(raw)\n",
    "                feautres_col.append(i)\n",
    "            \n",
    "    features = np.array(features, dtype='int32')\n",
    "    features_raw = np.array(features_raw, dtype='int32')\n",
    "    feautres_col = np.array(feautres_col, dtype='int32')\n",
    "    return csr_matrix((features, (features_raw, feautres_col)), shape=(raw + 1, total_features * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(q1s, prefixs=[]):\n",
    "    features = []\n",
    "    for raw, q1 in enumerate(q1s):\n",
    "        q1_with_pattern = 0\n",
    "        \n",
    "        for i, prefix in enumerate(prefixs):\n",
    "            t_q1, t_q2 = False, False\n",
    "            if prefix in q1:\n",
    "                t_q1 = True\n",
    "                q1_with_pattern = 1\n",
    "            \n",
    "        # features - both no pattern\n",
    "        features.append(0 if q1_with_pattern else 1)\n",
    "    return np.array(features, dtype='int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/\"\n",
    "is_train = True\n",
    "\n",
    "logger.info('Loading data')\n",
    "train = pd.read_csv(path + \"train.csv\")\n",
    "test = pd.read_csv(path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Clean and format data\n"
     ]
    }
   ],
   "source": [
    "logger.info('Clean and format data')\n",
    "train['ct_clean'] = train['comment_text'].astype(str).apply(lambda x: stem_str(x))\n",
    "\n",
    "train_q1 = train['ct_clean'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Mining pattern for first 1 among 159571\n",
      "INFO:__main__:\tfind 50\n",
      "INFO:__main__:Mining pattern for first 2 among 159571\n",
      "INFO:__main__:\tfind 50\n",
      "INFO:__main__:Mining pattern for first 3 among 159571\n",
      "INFO:__main__:\tfind 50\n",
      "INFO:__main__:Mining pattern for first 4 among 159571\n",
      "INFO:__main__:\tfind 50\n",
      "INFO:__main__:Mining pattern for first 5 among 159571\n",
      "INFO:__main__:\tfind 50\n"
     ]
    }
   ],
   "source": [
    "if is_train:\n",
    "    sentences = train_q1 \n",
    "    sentences = preprocess(sentences)\n",
    "    mining_pattern(sentences, 5, 50, '../cache/pattern.txt', less2backup=3)\n",
    "else:\n",
    "    prefixs = load_patterns('../cache/pattern.txt')\n",
    "    train_q1 = preprocess(train_q1)\n",
    "    \n",
    "    logger.info('Gen one hot feature for training data')\n",
    "    train_oh_features = gen_feature_one_hot(train_q1, prefixs)\n",
    "    logger.info('Gen feature for training data')\n",
    "    train_features = gen_features(train_q1, prefixs)\n",
    "    logger.info('Back up features')\n",
    "    with open('../cache/train.pattern.onehot.pkl', 'wb') as fo:\n",
    "        pickle.dump(train_oh_features, fo)\n",
    "    with open('../cache/train.pattern.pkl', 'wb') as fo:\n",
    "        pickle.dump(train_features, fo)\n",
    "    # test\n",
    "    logger.info('Clean and format test data')\n",
    "    test['ct_clean'] = test['comment_text'].astype(str).apply(lambda x: stem_str(x))\n",
    "    \n",
    "    test_ct = test['ct_clean'].tolist()\n",
    "    \n",
    "    test_q1 = preprocess(test_ct)\n",
    "    \n",
    "    logger.info('Gen one hot feature for test data')\n",
    "    test_oh_features = gen_feature_one_hot(test_q1, prefixs)\n",
    "    logger.info('Gen feature for test data')\n",
    "    test_features = gen_features(test_q1, prefixs)\n",
    "    logger.info('Back up features')\n",
    "    with open('../cache/test.pattern.onehot.pkl', 'wb') as fo:\n",
    "        pickle.dump(test_oh_features, fo)\n",
    "    with open('../cache/test.pattern.pkl', 'wb') as fo:\n",
    "        pickle.dump(test_features, fo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktc",
   "language": "python",
   "name": "ktc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
