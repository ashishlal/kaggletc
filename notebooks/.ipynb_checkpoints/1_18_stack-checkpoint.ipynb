{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../data/glove.840B.300d.txt'\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = train['toxic'].values\n",
    "y2 = train['severe_toxic'].values\n",
    "y3 = train['obscene'].values\n",
    "y4 = train['threat'].values\n",
    "y5 = train['insult'].values\n",
    "y6 = train['identity_hate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=100000\n",
    "maxlen=150\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Attention import *\n",
    "\n",
    "def build_model_nn1(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NN1(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN1_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn1(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "\n",
    "def build_model_nn2(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = AttentionWithContext()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att]) \n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = Dropout(0.1)(x)\n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN2_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn2(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "from Attention import *\n",
    "\n",
    "def build_model_nn3(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att1 = AttentionWithContext()(x)\n",
    "    att2 = Attention()(x)\n",
    "    x = concatenate([avg_pool, max_pool, att1, att2]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN3(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN3_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn3(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionWithContext import *\n",
    "from Attention import *\n",
    "\n",
    "def build_model_nn4(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                    num_sp_dr=0.2, optim=Adam(lr=1e-3)):\n",
    "    sequence_input = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    x = Conv1D(num_conv, kernel_size = kernel_size, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool]) \n",
    "    preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optim,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN4(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_conv=64, kernel_size=3, \n",
    "                 num_sp_dr=0.2, batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        self.num_conv = num_conv\n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN4_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn4(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_conv=self.num_conv, num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_nn5(num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_sp_dr=0.2, \n",
    "                    optim=Adam(lr=0.001)):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(num_sp_dr)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(num_lstm_gru_units, return_sequences=True,dropout=dr_lstm,recurrent_dropout=dr_rec))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optim,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN5(BaseEstimator):\n",
    "    def __init__(self,num_lstm_gru_units=128, dr_lstm=0.1, dr_rec=0.1, num_sp_dr=0.2,\n",
    "                 batch_size=128, optim=Adam(lr=1e-3)):\n",
    "        self.num_lstm_gru_units = num_lstm_gru_units\n",
    "        self.dr_lstm = dr_lstm\n",
    "        self.dr_rec = dr_rec\n",
    "        \n",
    "        self.num_sp_dr = num_sp_dr\n",
    "        self.batch_size=batch_size\n",
    "        self.optim=optim\n",
    "    def fit(self,X,y):\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "        self.filepath=\"../cache/1_18_NN5_weights_base.best.hdf5\"\n",
    "        checkpoint = ModelCheckpoint(self.filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "        ra_val = RocAucEvaluation(validation_data=(X_val, y_val), interval = 1)\n",
    "        callbacks_list = [ra_val,checkpoint, early]\n",
    "        model = build_model_nn5(num_lstm_gru_units=self.num_lstm_gru_units, dr_lstm=self.dr_lstm, \n",
    "                                dr_rec=self.dr_rec, \n",
    "                                num_sp_dr=self.num_sp_dr, optim=self.optim)\n",
    "        model.fit(X_tra, y_tra, batch_size=self.batch_size, epochs=5, validation_data=(X_val, y_val),\n",
    "          callbacks = callbacks_list,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        #Loading model weights\n",
    "        self.model.load_weights(self.filepath)\n",
    "        print('Predicting....')\n",
    "        y_pred = self.model.predict(X,batch_size=1024,verbose=1)\n",
    "        print(y_pred.shape)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((10,6))\n",
    "c = np.random.random((10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((10, 6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[:,:,0] = a\n",
    "b[:,:,1] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17727482, 0.45456644, 0.49212607, 0.50721638, 0.50686121,\n",
       "        0.161148  ],\n",
       "       [0.20563254, 0.64988956, 0.32744716, 0.17360135, 0.35707627,\n",
       "        0.48103542],\n",
       "       [0.44689288, 0.19128629, 0.53710537, 0.41709411, 0.37870518,\n",
       "        0.49350407],\n",
       "       [0.31327662, 0.16441258, 0.38323538, 0.21965772, 0.29665035,\n",
       "        0.52717279],\n",
       "       [0.40057884, 0.22551249, 0.48584782, 0.37296569, 0.19768885,\n",
       "        0.57517161],\n",
       "       [0.43608877, 0.34935421, 0.22674936, 0.61298084, 0.02550516,\n",
       "        0.25646376],\n",
       "       [0.34728608, 0.28166745, 0.25873408, 0.46843744, 0.28097777,\n",
       "        0.28730284],\n",
       "       [0.34534859, 0.30243579, 0.34651773, 0.37009074, 0.27523783,\n",
       "        0.2359247 ],\n",
       "       [0.18054381, 0.51721435, 0.22347868, 0.22697491, 0.38112764,\n",
       "        0.18070692],\n",
       "       [0.47845215, 0.44294616, 0.19088375, 0.28891556, 0.27543041,\n",
       "        0.41902096]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "final_score = 0.0\n",
    "n_classes = 6\n",
    "class Ensemble1(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "#         folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "#                                                              random_state=2016).split(X, y))\n",
    "        \n",
    "        folds = list(KFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=233)\n",
    "        OOF_columns = []\n",
    "\n",
    "#         S_train = np.zeros((X.shape[0], len(self.base_models)) * n_classes)\n",
    "#         S_test = np.zeros((T.shape[0], len(self.base_models)) * n_classes)\n",
    "        \n",
    "        S_train = np.zeros((X.shape[0], n_classes, len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], n_classes, len(self.base_models)))\n",
    "        \n",
    "#         S_train = []\n",
    "#         S_test = []\n",
    "#         (159571, 6)\n",
    "#         (159571, 2)\n",
    "#         print(y.shape)\n",
    "        print(S_train.shape)\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], n_classes, self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                print(test_idx.shape)\n",
    "                print(train_idx.shape)\n",
    "                print(test_idx.shape[0])\n",
    "                print(train_idx.shape[0])\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "#                 S_train[0:test_idx[0], n_classes, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_train[j*test_idx.shape[0]:(j+1)*test_idx.shape[0], :, i] = clf.predict_proba(X_holdout)\n",
    "                S_test_i[:, :, j] = clf.predict_proba(T)                \n",
    "            S_test[:, :, i] = S_test_i.mean(axis=2)\n",
    "            \n",
    "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,:,i])))\n",
    "#             print(\"  Base model_%d accuracy score: %.5f\\n\" % (i+1, accuracy_score(y, S_train[:,:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "#         OOF_S_train = pd.DataFrame(np.array(S_train), columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\n",
    "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,\n",
    "#                                                             accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "#             print(\"2nd level model final accuracy score: %.5f\" % (accuracy_score(y, stack_score)))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))    \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "#             print(\"2nd level models final accuracy score: %.5f\" % (accuracy_score(y, stack_score.mean(axis=1))))\n",
    "            final_score = accuracy_score(y, stack_score.mean(axis=1))\n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "final_score = 0.0\n",
    "class Ensemble(object):    \n",
    "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\n",
    "        self.mode = mode\n",
    "        self.n_splits = n_splits\n",
    "        self.stacker_2 = stacker_2\n",
    "        self.stacker_1 = stacker_1\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
    "                                                             random_state=2016).split(X, y))\n",
    "        \n",
    "        OOF_columns = []\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):                \n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "\n",
    "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                S_train[test_idx, i] = clf.predict_proba(X_holdout)[:,1]  \n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]                \n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "            \n",
    "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\n",
    "#             print(\"  Base model_%d accuracy score: %.5f\\n\" % (i+1, accuracy_score(y, S_train[:,i])))\n",
    "        \n",
    "            OOF_columns.append('Base model_'+str(i+1))\n",
    "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\n",
    "        print('\\n')\n",
    "        print('Correlation between out-of-fold predictions from Base models:')\n",
    "        print('\\n')\n",
    "        print(OOF_S_train.corr())\n",
    "        print('\\n')\n",
    "            \n",
    "        \n",
    "        if self.mode==1:\n",
    "            \n",
    "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\n",
    "                                                                   random_state=2016).split(S_train, y))\n",
    "            \n",
    "            OOF_columns = []\n",
    "\n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\n",
    "\n",
    "                for j, (train_idx, test_idx) in enumerate(folds_2):\n",
    "                    X_train_2 = S_train[train_idx]\n",
    "                    y_train_2 = y[train_idx]\n",
    "                    X_holdout_2 = S_train[test_idx]\n",
    "\n",
    "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(X_train_2, y_train_2)\n",
    "                                 \n",
    "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "                \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\n",
    "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,\n",
    "#                                                             accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                OOF_columns.append('1st level model_'+str(i+1))\n",
    "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between out-of-fold predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(OOF_S_train.corr())\n",
    "            print('\\n')\n",
    "\n",
    "\n",
    "        if self.mode==2:\n",
    "            \n",
    "            WOC_columns = []\n",
    "        \n",
    "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\n",
    "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\n",
    "               \n",
    "            for i, clf in enumerate(self.stacker_1):\n",
    "            \n",
    "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\n",
    "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\n",
    "                                       \n",
    "                for j in range(S_train.shape[1]):\n",
    "                                \n",
    "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\n",
    "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\n",
    "                                               \n",
    "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\n",
    "                    clf.fit(S_tr, y)\n",
    "\n",
    "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \n",
    "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\n",
    "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \n",
    "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\n",
    "            \n",
    "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, S_train_2.mean(axis=1))))\n",
    "#                 print(\"  1st level model_%d accuracy score: %.5f\\n\"%(i+1,accuracy_score(y, S_train_2.mean(axis=1))))\n",
    "                \n",
    "                \n",
    "                WOC_columns.append('1st level model_'+str(i+1))\n",
    "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between without-one-column predictions from 1st level models:')\n",
    "            print('\\n')\n",
    "            print(WOC_S_train.corr())\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            num_models = len(self.stacker_2)\n",
    "            if self.stacker_2==(et_model):\n",
    "                num_models=1\n",
    "        except TypeError:\n",
    "            num_models = len([self.stacker_2])\n",
    "            \n",
    "        if num_models==1:\n",
    "                \n",
    "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\n",
    "            self.stacker_2.fit(S_train_2, y)\n",
    "            \n",
    "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\n",
    "        \n",
    "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\n",
    "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\n",
    "#             print(\"2nd level model final accuracy score: %.5f\" % (accuracy_score(y, stack_score)))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))    \n",
    "        else:\n",
    "            \n",
    "            F_columns = []\n",
    "            \n",
    "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\n",
    "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\n",
    "            \n",
    "            for i, clf in enumerate(self.stacker_2):\n",
    "                \n",
    "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\n",
    "                clf.fit(S_train_2, y)\n",
    "                \n",
    "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\n",
    "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\n",
    "                \n",
    "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\n",
    "                \n",
    "                F_columns.append('2nd level model_'+str(i+1))\n",
    "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\n",
    "            print('\\n')\n",
    "            print('Correlation between final predictions from 2nd level models:')\n",
    "            print('\\n')\n",
    "            print(F_S_train.corr())\n",
    "            print('\\n')\n",
    "        \n",
    "            stack_res = res.mean(axis=1)            \n",
    "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\n",
    "#             print(\"2nd level models final accuracy score: %.5f\" % (accuracy_score(y, stack_score.mean(axis=1))))\n",
    "            final_score = roc_auc_score(y, stack_score.mean(axis=1))\n",
    "        return stack_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1 = NN1()\n",
    "nn2 = NN2()\n",
    "nn3 = NN3()\n",
    "nn4 = NN4()\n",
    "nn5 = NN5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacker models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "et_model = ExtraTreesClassifier(n_estimators=200, max_depth=6, min_samples_split=10, random_state=10)\n",
    "\n",
    "mlp_model = MLPClassifier(max_iter=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit NN2_1 fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9812\n",
      " ROC-AUC - epoch: 1 - score: 0.987727\n",
      "Epoch 00000: val_acc improved from -inf to 0.98256, saving model to ../cache/1_18_NN2_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 749s - loss: 0.0518 - acc: 0.9812 - val_loss: 0.0448 - val_acc: 0.9826\n",
      "Predicting....\n",
      "79786/79786 [==============================] - 163s   \n",
      "(79786, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 318s   \n",
      "(153164, 6)\n",
      "Fit NN2_1 fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9808\n",
      " ROC-AUC - epoch: 1 - score: 0.987395\n",
      "Epoch 00000: val_acc improved from -inf to 0.98286, saving model to ../cache/1_18_NN2_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 770s - loss: 0.0522 - acc: 0.9808 - val_loss: 0.0434 - val_acc: 0.9829\n",
      "Predicting....\n",
      "79785/79785 [==============================] - 160s   \n",
      "(79785, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 308s   \n",
      "(153164, 6)\n",
      "  Base model_1 score: 0.97064\n",
      "\n",
      "Fit NN1_2 fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9807\n",
      " ROC-AUC - epoch: 1 - score: 0.987837\n",
      "Epoch 00000: val_acc improved from -inf to 0.98288, saving model to ../cache/1_18_NN1_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 788s - loss: 0.0527 - acc: 0.9807 - val_loss: 0.0444 - val_acc: 0.9829\n",
      "Predicting....\n",
      "79786/79786 [==============================] - 161s   \n",
      "(79786, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 312s   \n",
      "(153164, 6)\n",
      "Fit NN1_2 fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/1\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809\n",
      " ROC-AUC - epoch: 1 - score: 0.986556\n",
      "Epoch 00000: val_acc improved from -inf to 0.98350, saving model to ../cache/1_18_NN1_weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 760s - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0433 - val_acc: 0.9835\n",
      "Predicting....\n",
      "79785/79785 [==============================] - 159s   \n",
      "(79785, 6)\n",
      "Predicting....\n",
      "153164/153164 [==============================] - 310s   \n",
      "(153164, 6)\n",
      "  Base model_2 score: 0.96510\n",
      "\n",
      "\n",
      "\n",
      "Correlation between out-of-fold predictions from Base models:\n",
      "\n",
      "\n",
      "              Base model_1  Base model_2\n",
      "Base model_1      1.000000      0.894376\n",
      "Base model_2      0.894376      1.000000\n",
      "\n",
      "\n",
      "Fit LogisticRegression_1 subset 1\n",
      "Fit LogisticRegression_1 subset 2\n",
      "  1st level model_1 score: 0.97226\n",
      "\n",
      "Fit ExtraTreesClassifier_2 subset 1\n",
      "Fit ExtraTreesClassifier_2 subset 2\n",
      "  1st level model_2 score: 0.97249\n",
      "\n",
      "Fit MLPClassifier_3 subset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit MLPClassifier_3 subset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1st level model_3 score: 0.97237\n",
      "\n",
      "\n",
      "\n",
      "Correlation between without-one-column predictions from 1st level models:\n",
      "\n",
      "\n",
      "                   1st level model_1  1st level model_2  1st level model_3\n",
      "1st level model_1           1.000000           0.939246           0.871376\n",
      "1st level model_2           0.939246           1.000000           0.984974\n",
      "1st level model_3           0.871376           0.984974           1.000000\n",
      "\n",
      "\n",
      "Fit LogisticRegression_1\n",
      "  2nd level model_1 score: 0.97230\n",
      "\n",
      "Fit ExtraTreesClassifier_2\n",
      "  2nd level model_2 score: 0.97246\n",
      "\n",
      "\n",
      "\n",
      "Correlation between final predictions from 2nd level models:\n",
      "\n",
      "\n",
      "                   2nd level model_1  2nd level model_2\n",
      "2nd level model_1           1.000000           0.985908\n",
      "2nd level model_2           0.985908           1.000000\n",
      "\n",
      "\n",
      "2nd level models final score: 0.97241\n"
     ]
    }
   ],
   "source": [
    "# Mode 2 run\n",
    "stack = Ensemble(mode=2,\n",
    "        n_splits=2,\n",
    "        stacker_2 = (log_model, et_model),         \n",
    "        stacker_1 = (log_model, et_model, mlp_model),\n",
    "        base_models = (\n",
    "            nn3,nn4,nn5\n",
    "            \n",
    "        ))       \n",
    "        \n",
    "y_pred1 = stack.fit_predict(x_train, y1, x_test)\n",
    "# y_pred2 = stack.fit_predict(x_train, y2, x_test)\n",
    "# y_pred3 = stack.fit_predict(x_train, y3, x_test)\n",
    "# y_pred4 = stack.fit_predict(x_train, y4, x_test)\n",
    "# y_pred5 = stack.fit_predict(x_train, y5, x_test)\n",
    "# y_pred6 = stack.fit_predict(x_train, y6, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission from mode 2\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission[list_classes] = y_pred\n",
    "sample_submission.to_csv('../submission/sub_1_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train = np.zeros((x_train.shape[0], x_train.shape[1], 2))\n",
    "# S_test = np.zeros((T.shape[0], T.shape[1], len(self.base_models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.zeros((79786, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train[x1.shape[0],x1.shape[1],0] = x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktc",
   "language": "python",
   "name": "ktc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
