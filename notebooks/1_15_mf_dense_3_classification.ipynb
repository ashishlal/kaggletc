{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import KFold,train_test_split,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "# from generate_selftrained_w2v import gen_w2v\n",
    "# from config import path\n",
    "\n",
    "path = '../cache/'\n",
    "seed = 1024\n",
    "np.random.seed(seed)\n",
    "# path = '../input/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats= ['comment_text_unigram','comment_text_bigram']\n",
    "X = []\n",
    "for f in feats:\n",
    "    X.append(pd.read_pickle(path+'train_%s_tfidf_nmf.pkl'%f))\n",
    "    X.append(pd.read_pickle(path+'train_%s_tfidf_svd.pkl'%f))\n",
    "# X.append(np.loadtxt(path+'train_spacy_diff_pretrained.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3= [\n",
    "'comment_text',\n",
    "]\n",
    "for f in feats3:\n",
    "    X.append(pd.read_pickle(path+'train_%s_lda.pkl'%f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack(X)#.tocsr()\n",
    "len_train = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram_features = pd.read_csv(path+'train_unigram_features.csv').values\n",
    "train_bigram_features = pd.read_csv(path+'train_bigram_features.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_porter_stop_cuss_features = pd.read_csv(path+'train_porter_stop_cuss_features.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idf_stats_features = pd.read_csv(path+'train_idf_stats_features.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pattern = pd.read_pickle(path+'train.pattern.pkl').reshape((X.shape[0],3))\n",
    "train_pattern = pd.read_pickle(path+'train.pattern.pkl').reshape((X.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 1)\n",
      "(159571, 50)\n"
     ]
    }
   ],
   "source": [
    "print(train_pattern.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "train_count_ct_mark_all = train['comment_text'].astype(str).apply( lambda x : x.count(\"!\")+x.count(\"?\")+x.count(\",\")+x.count(\".\")+x.count(\"*\")+x.count(\"@\")+x.count(\"#\")+x.count(\"$\")+x.count(\"%\")+x.count(\"^\")+x.count(\"&\")).values.reshape(-1,1)\n",
    "train_count_ct_mark_bang = train['comment_text'].astype(str).apply( lambda x : x.count(\"!\")).values.reshape(-1,1)\n",
    "train_count_ct_mark_star = train['comment_text'].astype(str).apply( lambda x : x.count(\"*\")).values.reshape(-1,1)\n",
    "train_count_ct_mark_special = train['comment_text'].astype(str).apply( lambda x : x.count(\"!\")+x.count(\"@\")+x.count(\"#\")+x.count(\"$\")+x.count(\"%\")+x.count(\"^\")+x.count(\"&\")+x.count(\"*\")).values.reshape(-1,1)\n",
    "\n",
    "train_ratio_bang_mark = (train_count_ct_mark_bang)/(train_count_ct_mark_all+1e-7)\n",
    "train_ratio_star_mark = (train_count_ct_mark_star)/(train_count_ct_mark_all+1e-7)\n",
    "train_ratio_special_mark = (train_count_ct_mark_special)/(train_count_ct_mark_all+1e-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/watts/anaconda3/envs/ktc/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_char_length = train['comment_text'].apply(lambda x: len(str(x))).reshape(-1,1)\n",
    "train_num_words = train['comment_text'].map(lambda x: len(str(x).split(' '))).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entropy_unigram = pd.read_csv(path+'train_entropy_unigram.csv').values\n",
    "train_entropy_bigram = pd.read_csv(path+'train_entropy_bigram.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 2)\n",
      "(159571, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_entropy_bigram.shape)\n",
    "print(train_entropy_unigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 5)\n",
      "(159571, 5)\n",
      "(159571, 4)\n",
      "(159571, 5)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 1)\n",
      "(159571, 2)\n",
      "(159571, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_unigram_features.shape)\n",
    "print(train_bigram_features.shape)\n",
    "print(train_porter_stop_cuss_features.shape)\n",
    "\n",
    "print(train_idf_stats_features.shape)\n",
    "\n",
    "print(train_pattern.shape)\n",
    "\n",
    "print(train_count_ct_mark_all.shape)\n",
    "print(train_count_ct_mark_bang.shape)\n",
    "print(train_count_ct_mark_star.shape)\n",
    "print(train_count_ct_mark_special.shape)\n",
    "print(train_ratio_bang_mark.shape)\n",
    "print(train_ratio_star_mark.shape)\n",
    "print(train_ratio_special_mark.shape)\n",
    "print(train_char_length.shape)\n",
    "print(train_num_words.shape)\n",
    "print(train_entropy_unigram.shape)\n",
    "print(train_entropy_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic_features = np.hstack([\n",
    "    train_unigram_features,\n",
    "    train_bigram_features,\n",
    "    train_porter_stop_cuss_features,\n",
    "    \n",
    "    train_idf_stats_features,\n",
    "    \n",
    "    train_pattern,\n",
    "    \n",
    "    train_count_ct_mark_all,\n",
    "    train_count_ct_mark_bang,\n",
    "    train_count_ct_mark_star,\n",
    "    train_count_ct_mark_special,\n",
    "    train_ratio_bang_mark,\n",
    "    train_ratio_star_mark,\n",
    "    train_ratio_special_mark,\n",
    "    train_char_length,\n",
    "    train_num_words,\n",
    "    train_entropy_unigram,\n",
    "    train_entropy_bigram\n",
    "    \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([X,train_basic_features])#.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/X', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = train_df['toxic'].values\n",
    "y2 = train_df['severe_toxic'].values\n",
    "y3 = train_df['obscene'].values\n",
    "y4 = train_df['threat'].values\n",
    "y5 = train_df['insult'].values\n",
    "y6 = train_df['identity_hate'].values\n",
    "y7 = train_df.loc[:,['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/y1', y1)\n",
    "np.save('../cache/y2', y2)\n",
    "np.save('../cache/y3', y3)\n",
    "np.save('../cache/y4', y4)\n",
    "np.save('../cache/y5', y5)\n",
    "np.save('../cache/y6', y6)\n",
    "np.save('../cache/y7', y7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_unigram_features\n",
    "del train_bigram_features\n",
    "del train_porter_stop_cuss_features\n",
    "\n",
    "del train_idf_stats_features\n",
    "\n",
    "del train_count_ct_mark_all\n",
    "del train_count_ct_mark_bang\n",
    "del train_count_ct_mark_star\n",
    "del train_count_ct_mark_special\n",
    "del train_ratio_bang_mark\n",
    "del train_ratio_star_mark\n",
    "del train_ratio_special_mark\n",
    "del train_char_length\n",
    "del train_num_words\n",
    "del train_entropy_unigram\n",
    "del train_entropy_bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 83)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "813"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "153164-152351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 4)\n",
      "(153164, 16)\n",
      "(153164, 4)\n",
      "(153164, 16)\n",
      "(153164, 10)\n"
     ]
    }
   ],
   "source": [
    "feats= ['comment_text_unigram','comment_text_bigram']\n",
    "X_t = []\n",
    "for f in feats:\n",
    "    X_t.append(pd.read_pickle(path+'test_%s_tfidf_nmf.pkl'%f))\n",
    "    X_t.append(pd.read_pickle(path+'test_%s_tfidf_svd.pkl'%f))\n",
    "\n",
    "# feats2= [\n",
    "#     'comment_text',\n",
    "#     'comment_text_porter',\n",
    "# ]\n",
    "# for f in feats2:\n",
    "#     X_t.append(pd.read_pickle(path+'test_%s_nmf.pkl'%f))\n",
    "#     X_t.append(pd.read_pickle(path+'test_%s_svd.pkl'%f))\n",
    "\n",
    "feats3= [\n",
    "    'comment_text',\n",
    "]\n",
    "for f in feats3:\n",
    "    X_t.append(pd.read_pickle(path+'test_%s_lda.pkl'%f))\n",
    "\n",
    "for l in X_t:\n",
    "    print(l.shape)\n",
    "X_t = np.hstack(X_t)#.tocsr()\n",
    "len_test = X_t.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_unigram_features = pd.read_csv(path+'test_unigram_features.csv').values\n",
    "test_bigram_features = pd.read_csv(path+'test_bigram_features.csv').values\n",
    "test_porter_stop_cuss_features = pd.read_csv(path+'test_porter_stop_cuss_features.csv').values\n",
    "\n",
    "test_idf_stats_features = pd.read_csv(path+'test_idf_stats_features.csv').values\n",
    "test_pattern = pd.read_pickle(path+'test.pattern.pkl').reshape((X_t.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test_count_ct_mark_all = test['comment_text'].astype(str).apply( lambda x : x.count(\"!\")+x.count(\"?\")+x.count(\",\")+x.count(\".\")+x.count(\"*\")+x.count(\"@\")+x.count(\"#\")+x.count(\"$\")+x.count(\"%\")+x.count(\"^\")+x.count(\"&\")).values.reshape(-1,1)\n",
    "test_count_ct_mark_bang = test['comment_text'].astype(str).apply( lambda x : x.count(\"!\")).values.reshape(-1,1)\n",
    "test_count_ct_mark_star = test['comment_text'].astype(str).apply( lambda x : x.count(\"*\")).values.reshape(-1,1)\n",
    "test_count_ct_mark_special = test['comment_text'].astype(str).apply( lambda x : x.count(\"!\")+x.count(\"@\")+x.count(\"#\")+x.count(\"$\")+x.count(\"%\")+x.count(\"^\")+x.count(\"&\")+x.count(\"*\")).values.reshape(-1,1)\n",
    "\n",
    "test_ratio_bang_mark = (test_count_ct_mark_bang)/(test_count_ct_mark_all+1e-7)\n",
    "test_ratio_star_mark = (test_count_ct_mark_star)/(test_count_ct_mark_all+1e-7)\n",
    "test_ratio_special_mark = (test_count_ct_mark_special)/(test_count_ct_mark_all+1e-7)\n",
    "\n",
    "test_char_length = test['comment_text'].apply(lambda x: len(str(x))).values.reshape(-1,1)\n",
    "test_num_words = test['comment_text'].map(lambda x: len(str(x).split(' '))).values.reshape(-1,1)\n",
    "\n",
    "test_entropy_unigram = pd.read_csv(path+'test_entropy_unigram.csv')\n",
    "test_entropy_bigram = pd.read_csv(path+'test_entropy_bigram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 5)\n",
      "(153164, 5)\n",
      "(153164, 4)\n",
      "(153164, 5)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 1)\n",
      "(153164, 2)\n",
      "(153164, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test_unigram_features.shape)\n",
    "print(test_bigram_features.shape)\n",
    "print(test_porter_stop_cuss_features.shape)\n",
    "\n",
    "print(test_idf_stats_features.shape)\n",
    "\n",
    "print(test_pattern.shape)\n",
    "\n",
    "print(test_count_ct_mark_all.shape)\n",
    "print(test_count_ct_mark_bang.shape)\n",
    "print(test_count_ct_mark_star.shape)\n",
    "print(test_count_ct_mark_special.shape)\n",
    "print(test_ratio_bang_mark.shape)\n",
    "print(test_ratio_star_mark.shape)\n",
    "print(test_ratio_special_mark.shape)\n",
    "print(test_char_length.shape)\n",
    "print(test_num_words.shape)\n",
    "print(test_entropy_unigram.shape)\n",
    "print(test_entropy_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basic_features = np.hstack([\n",
    "    test_unigram_features,\n",
    "    test_bigram_features,\n",
    "    test_porter_stop_cuss_features,\n",
    "    \n",
    "    test_idf_stats_features,\n",
    "    \n",
    "    test_pattern,\n",
    "    \n",
    "    test_count_ct_mark_all,\n",
    "    test_count_ct_mark_bang,\n",
    "    test_count_ct_mark_star,\n",
    "    test_count_ct_mark_special,\n",
    "    test_ratio_bang_mark,\n",
    "    test_ratio_star_mark,\n",
    "    test_ratio_special_mark,\n",
    "    test_char_length,\n",
    "    test_num_words,\n",
    "    test_entropy_unigram,\n",
    "    test_entropy_bigram,\n",
    "    \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = np.hstack([X_t,test_basic_features])#.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../cache/X_t', X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_unigram_features\n",
    "del test_bigram_features\n",
    "del test_porter_stop_cuss_features\n",
    "\n",
    "del test_idf_stats_features\n",
    "\n",
    "del test_count_ct_mark_all\n",
    "del test_count_ct_mark_bang\n",
    "del test_count_ct_mark_star\n",
    "del test_count_ct_mark_special\n",
    "del test_ratio_bang_mark\n",
    "del test_ratio_star_mark\n",
    "del test_ratio_special_mark\n",
    "del test_char_length\n",
    "del test_num_words\n",
    "del test_entropy_unigram\n",
    "del test_entropy_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153164, 83)\n"
     ]
    }
   ],
   "source": [
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import make_mf_classification\n",
    "\n",
    "# X = pd.DataFrame(X).replace(np.nan,0).values\n",
    "# X_t = pd.DataFrame(X_t).replace(np.nan,0).values\n",
    "# X = pd.DataFrame(X).replace(np.inf,0).values\n",
    "# X_t = pd.DataFrame(X_t).replace(np.inf,0).values\n",
    "\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# clf = lgb.LGBMClassifier(\n",
    "#                         num_leaves=31,\n",
    "#                         learning_rate= 0.008,\n",
    "#                         n_estimators=26972,\n",
    "#                         subsample = 0.75,\n",
    "#                         colsample_bytree = 0.54,\n",
    "#                         min_child_weight=13,\n",
    "#                         min_split_gain=0.3,\n",
    "#                         )\n",
    "\n",
    "# make_mf_classification(X ,y, clf, X_t, n_folds=5,seed=seed,nb_epoch=1,max_features=1.0,name='lgb_3',path=path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(\n",
    "#             n_estimators=500,\n",
    "#             max_depth = 18,\n",
    "#             min_samples_split=16,\n",
    "#             bootstrap=False,\n",
    "#             max_features=0.1,\n",
    "#             n_jobs=8,\n",
    "#             random_state=seed,\n",
    "#             verbose=True,\n",
    "#             )\n",
    "\n",
    "# make_mf_classification(X ,y, clf, X_t, n_folds=5,seed=seed,nb_epoch=1,max_features=1.0,name='rf_3',path=path)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "# clf = ExtraTreesClassifier(\n",
    "#             n_estimators=500,\n",
    "#             max_depth = 23,\n",
    "#             min_samples_split=8,\n",
    "#             bootstrap=False,\n",
    "#             max_features=0.3,\n",
    "#             n_jobs=8,\n",
    "#             random_state=seed,\n",
    "#             verbose=True,\n",
    "#             )\n",
    "\n",
    "# make_mf_classification(X ,y, clf, X_t, n_folds=5,seed=seed,nb_epoch=1,max_features=1.0,name='et_3',path=path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = xgb.XGBClassifier(\n",
    "#         n_estimators=9326/3,\n",
    "#         learning_rate = 0.093/3,\n",
    "#         max_depth=8,\n",
    "#         subsample=0.75,\n",
    "#         colsample_bytree = 0.54,\n",
    "#         gamma = 0.3,\n",
    "#         reg_lambda=0,\n",
    "#         min_child_weight=13,\n",
    "#         seed = seed,\n",
    "#         )\n",
    "# make_mf_classification(X ,y, clf, X_t, n_folds=5,seed=seed,nb_epoch=1,max_features=1.0,name='xgb_3',path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Embedding, LSTM, Dense,Flatten, Dropout, merge,Convolution1D,MaxPooling1D,Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD,Nadam\n",
    "from keras.layers.advanced_activations import PReLU,LeakyReLU,ELU\n",
    "from keras.models import Model\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def build_model(maxlen=238,hidden=512):\n",
    "    inputs = []\n",
    "    inputs_q1 = Input(shape=(maxlen,),name='input_q1',sparse=False)\n",
    "    inputs.append(inputs_q1)\n",
    "\n",
    "    fc1 = Dense(hidden)(inputs_q1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.2)(fc1)\n",
    "\n",
    "    fc1 = Dense(hidden)(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.2)(fc1)\n",
    "\n",
    "    fc1 = Dense(hidden)(fc1)\n",
    "    fc1 = PReLU()(fc1)\n",
    "    fc1 = BatchNormalization()(fc1)\n",
    "    fc1 = Dropout(0.2)(fc1)\n",
    "\n",
    "    \n",
    "    outputs = Dense(1,activation='sigmoid')(fc1)\n",
    "    model = Model(input=inputs, output=outputs)\n",
    "\n",
    "    model.compile(\n",
    "                optimizer='adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "              )\n",
    "    return model\n",
    "\n",
    "class NN(BaseEstimator):\n",
    "    def __init__(self,input_shape,hidden=128):\n",
    "        self.input_shape = input_shape\n",
    "        self.hidden = hidden\n",
    "    def fit(self,X,y):\n",
    "        model = build_model(maxlen=self.input_shape,hidden=self.hidden)\n",
    "        model.fit(X,y,epochs=250,batch_size=128,shuffle=True,verbose=1)\n",
    "        self.model=model\n",
    "    def predict_proba(self,X):\n",
    "        y_p = self.model.predict(X,batch_size=1024).ravel()\n",
    "        y_p_inv= np.ones(y_p.shape[0])-y_p\n",
    "        y_p = np.hstack([y_p_inv.reshape(-1,1),y_p.reshape(-1,1)])\n",
    "        return y_p\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "clf = NN(X.shape[1],128)\n",
    "make_mf_classification(scaler.transform(X) ,y, clf, scaler.transform(X_t), n_folds=5,seed=seed,nb_epoch=1,max_features=1.0,name='nn_3',path=path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktc",
   "language": "python",
   "name": "ktc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
